@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

@inproceedings{amazonreviews,
author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
title = {Image-Based Recommendations on Styles and Substitutes},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767755},
doi = {10.1145/2766462.2767755},
pages = {43–52},
numpages = {10},
keywords = {recommender systems, metric learning, visual features},
location = {Santiago, Chile},
booktitle = {SIGIR 2015}
}

@techreport{gimpel2010,
  title={Part-of-speech tagging for {T}witter: Annotation, features, and experiments},
  author={Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A},
  year={2010},
  institution={Carnegie-Mellon Univ Pittsburgh Pa School of Computer Science}
}

@inproceedings{hovy2014gimpel,
    title = "Experiments with crowdsourced re-annotation of a {POS} tagging data set",
    author = "Hovy, Dirk  and
      Plank, Barbara  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2062",
    doi = "10.3115/v1/P14-2062",
    pages = "377--382",
}


@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}

@article{northcutt2021pervasive,
  title={Pervasive label errors in test sets destabilize machine learning benchmarks},
  author={Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  journal={arXiv:2103.14749},
  year={2021}
}


@inproceedings{gururangan2020tapt,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}


@InProceedings{joulin2017bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month={April},
  year={2017},
  publisher={Association for Computational Linguistics},
  pages={427--431},
}

@article{ng2021data,
  title={Data-Centric {AI} Competition},
  author={Ng, Andrew and Laird, Dillon and He, Lynn},
  journal={DeepLearning AI. Available online: https://https-deeplearning-ai. github. io/data-centric-comp/(accessed on 9 December 2021)},
  year={2021}
}

@inproceedings{hong2021nllp,
    title = "Learning from Limited Labels for Long Legal Dialogue",
    author = "Hong, Jenny  and
      Chong, Derek  and
      Manning, Christopher",
    booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nllp-1.20",
    doi = "10.18653/v1/2021.nllp-1.20",
    pages = "190--204",
    abstract = "We study attempting to achieve high accuracy information extraction of case factors from a challenging dataset of parole hearings, which, compared to other legal NLP datasets, has longer texts, with fewer labels. On this co pus, existing work directly applying pretrained neural models has failed to extract all but a few relatively basic items with little improvement over rule-based extraction. We address two challenges posed by existing work: training on long documents and reasoning over complex speech patterns. We use a similar approach to the two-step open-domain question answering approach by using a Reducer to extract relevant text segments and a Producer to generate both extractive answers and non-extractive classifications. In a context like ours, with limited labeled data, we show that a superior approach for strong performance within limited development time is to use a combination of a rule-based Reducer and a neural Producer. We study four representative tasks from the parole dataset. On all four, we improve extraction from the previous benchmark of 0.41{--}0.63 to 0.83{--}0.89 F1.",
}

@article{hendrycks2018using,
  title={Using trusted data to train deep networks on labels corrupted by severe noise},
  author={Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{xia2020extended,
  title={Extended {T}: Learning with mixed closed-set and open-set noisy labels},
  author={Xia, Xiaobo and Liu, Tongliang and Han, Bo and Wang, Nannan and Deng, Jiankang and Li, Jiatong and Mao, Yinian},
  journal={arXiv},
  year={2020}
}

@article{amid2019robust,
  title={Robust bi-tempered logistic loss based on {B}regman divergences},
  author={Amid, Ehsan and Warmuth, Manfred KK and Anil, Rohan and Koren, Tomer},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{liu2020peer,
  title={Peer loss functions: Learning from noisy labels without knowing noise rates},
  author={Liu, Yang and Guo, Hongyi},
  booktitle={International Conference on Machine Learning},
  pages={6226--6236},
  year={2020},
  organization={PMLR}
}

@inproceedings{ma2020normalized,
  title={Normalized loss functions for deep learning with noisy labels},
  author={Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Romano, Simone and Erfani, Sarah and Bailey, James},
  booktitle={International Conference on Machine Learning},
  pages={6543--6553},
  year={2020},
  organization={PMLR}
}

@article{azadi2015auxiliary,
  title={Auxiliary image regularization for deep {CNN}s with noisy labels},
  author={Azadi, Samaneh and Feng, Jiashi and Jegelka, Stefanie and Darrell, Trevor},
  journal={arXiv preprint arXiv:1511.07069},
  year={2015}
}

@article{bar2021multiplicative,
  title={Multiplicative Reweighting for Robust Neural Network Optimization},
  author={Bar, Noga and Koren, Tomer and Giryes, Raja},
  journal={arXiv preprint arXiv:2102.12192},
  year={2021}
}
@inproceedings{zhou-chen-2021-learning,
    title = "Learning from Noisy Labels for Entity-Centric Information Extraction",
    author = "Zhou, Wenxuan  and
      Chen, Muhao",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.437",
    doi = "10.18653/v1/2021.emnlp-main.437",
    pages = "5381--5392",
    abstract = "Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research.",
}

@article{song2022survey,
  title={Learning from Noisy Labels with Deep Neural Networks: A Survey},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Jae-Gil Lee},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022}}
  
@inproceedings{zhang-stratos-2021-understanding,
title = "Understanding Hard Negatives in Noise Contrastive Estimation",
author = "Zhang, Wenzheng  and
  Stratos, Karl",
booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
month = jun,
year = "2021",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2021.naacl-main.86",
doi = "10.18653/v1/2021.naacl-main.86",
pages = "1090--1101",
abstract = "The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives{---}highest-scoring incorrect examples under the model{---}are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.",
}

@article{kumar2021constrained,
  title={Constrained Instance and Class Reweighting for Robust Learning under Label Noise},
  author={Kumar, Abhishek and Amid, Ehsan},
  journal={arXiv},
  year={2021}
}

@inproceedings{gordon2021metrics,
author = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445423},
doi = {10.1145/3411764.3445423},
abstract = { Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people’s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {388},
numpages = {14},
location = {Yokohama, Japan},
series = {CHI '21}
}


@article{soh2021building,
  title={Building Legal Datasets},
  author={Soh, Jerrold},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{bartlfeminist,
  title={Feminist Curation of Text for Data-centric AI},
  author={Bartl, Marion and Leavy, Susan},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{nicolichcomparing,
  title={Comparing Data Augmentation and Annotation Standardization to Improve End-to-end Spoken Language Understanding Models},
  author={Nicolich-Henkin, Leah and Nakatani, Taichi and Trozenski, Zach and Whiteman, Joel and Susanj, Nathan},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{kangfinding,
  title={Finding Label Errors in Autonomous Vehicle Data With Learned Observation Assertions},
  author={Kang, Daniel and Arechiga, Nikos and Pillai, Sudeep and Bailis, Peter and Zaharia, Matei},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{leaounleashing,
  title={Unleashing the Power of Industrial Big Data through Scalable Manual Labeling},
  author={Leao, Bruno P and Fradkin, Dmitriy and Lan, Tu and Wang, Jianhui},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{neurisdcai,
  title={{NeurIPS} Data-Centric {AI} Workshop},
  author={{DCAI Workshop}},
  url={https://datacentricai.org/neurips21/},
  journal={NeurIPS 2021 Data-Centric AI Workshop},
  year={2021}
}

@article{crfm-adaptation,
  title={Adaptation},
  author={Li, Xiang Lisa and Mitchell, Eric and Xie, Sang Michael and Li, Xuechen and Hashimoto, Tatsunori},
  journal={arXiv:2108.07258},
  booktitle = {On the opportunities and risks of foundation models},
  pages = {85--90},
  year={2021}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{wheway2000using,
  title={Using boosting to detect noisy data},
  author={Wheway, Virginia},
  booktitle={Pacific Rim International Conference on Artificial Intelligence},
  pages={123--130},
  year={2000},
  organization={Springer}
}

@article{sluban2014ensemble,
  title={Ensemble-based noise detection: noise ranking and visual performance evaluation},
  author={Sluban, Borut and Gamberger, Dragan and Lavra{\v{c}}, Nada},
  journal={Data mining and knowledge discovery},
  volume={28},
  number={2},
  pages={265--303},
  year={2014},
  publisher={Springer}
}

@article{delany2012profiling,
  title={Profiling instances in noise reduction},
  author={Delany, Sarah Jane and Segata, Nicola and Mac Namee, Brian},
  journal={Knowledge-Based Systems},
  volume={31},
  pages={28--40},
  year={2012},
  publisher={Elsevier}
}

@article{gamberger2000noise,
  title={Noise detection and elimination in data preprocessing: experiments in medical domains},
  author={Gamberger, Dragan and Lavrac, Nada and Dzeroski, Saso},
  journal={Applied artificial intelligence},
  volume={14},
  number={2},
  pages={205--223},
  year={2000},
  publisher={Taylor \& Francis}
}

@inproceedings{thongkam2008support,
  title={Support vector machine for outlier detection in breast cancer survivability prediction},
  author={Thongkam, Jaree and Xu, Guandong and Zhang, Yanchun and Huang, Fuchun},
  booktitle={Asia-Pacific Web Conference},
  pages={99--109},
  year={2008},
  organization={Springer}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}

@article{kim2021fine,
  title={{FINE} Samples for Learning with Noisy Labels},
  author={Kim, Taehyeon and Ko, Jongwoo and Choi, JinHwan and Yun, Se-Young and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{huang2019o2u,
  title={O2u-net: A simple noisy label detection approach for deep neural networks},
  author={Huang, Jinchi and Qu, Lie and Jia, Rongfei and Zhao, Binqiang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3326--3334},
  year={2019}
}

@mastersthesis{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  school={U. Toronto}
}

@article{vinyals2016matching,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{chen2021detecting,
  title={Detecting errors and estimating accuracy on unlabeled data with self-training ensembles},
  author={Chen, Jiefeng and Liu, Frederick and Avci, Besim and Wu, Xi and Liang, Yingyu and Jha, Somesh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{reiss2020identifying,
  title={Identifying incorrect labels in the {CoNLL}-2003 corpus},
  author={Reiss, Frederick and Xu, Hong and Cutler, Bryan and Muthuraman, Karthik and Eichenberger, Zachary},
  booktitle={Proceedings of the 24th conference on computational natural language learning},
  pages={215--226},
  year={2020}
}

@inproceedings{conll2003,
author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
title = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119176.1119195},
doi = {10.3115/1119176.1119195},
abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4},
pages = {142–147},
numpages = {6},
location = {Edmonton, Canada},
series = {CONLL '03}
}


@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@ARTICLE{frenay2014survey, author={Frenay, Benoit and Verleysen, Michel},  journal={IEEE Transactions on Neural Networks and Learning Systems},   title={Classification in the Presence of Label Noise: A Survey},   year={2014},  volume={25},  number={5},  pages={845-869},  doi={10.1109/TNNLS.2013.2292894}}

@article{algan2020label,
  title={Label noise types and their effects on deep learning},
  author={Algan, G{\"o}rkem and Ulusoy, Ilkay},
  journal={arXiv:2003.10471},
  year={2020}
}

@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv:1705.10694},
  year={2017}
}

@article{xu2021dataclue,
  title={{DataCLUE}: A Benchmark Suite for Data-centric {NLP}},
  author={Xu, Liang and Liu, Jiacheng and Pan, Xiang and Lu, Xiaojing and Hou, Xiaofeng},
  journal={arXiv:2111.08647},
  year={2021}
}

@inproceedings{desai-durrett-2020-calibration,
    title = "Calibration of Pre-trained Transformers",
    author = "Desai, Shrey  and
      Durrett, Greg",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.21",
    doi = "10.18653/v1/2020.emnlp-main.21",
    pages = "295--302",
    abstract = "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models{'} posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
}

@article{karthik2021learning,
  title={Learning from long-tailed data with noisy labels},
  author={Karthik, Shyamgopal and Revaud, J{\'e}rome and Chidlovskii, Boris},
  journal={arXiv:2108.11096},
  year={2021}
}

@article{agley2021quality,
  title={Quality control questions on {A}mazon’s {M}echanical {T}urk ({MTurk}): A randomized trial of impact on the {USAUDIT}, {PHQ}-9, and {GAD}-7},
  author={Agley, Jon and Xiao, Yunyu and Nolan, Rachael and Golzarri-Arroyo, Lilian},
  journal={Behavior research methods},
  pages={1--13},
  year={2021},
  publisher={Springer}
}

@article{mturkbots,
  title={After the Bot Scare: Understanding What’s Been Happening With Data Collection on {MTurk} and How to Stop It},
  author={Moss, Aaron and Litman, Leib},
  journal={CloudResearch},
  year={2018},
  url={https://www.cloudresearch.com/resources/blog/after-the-bot-scare-understanding-whats-been-happening-with-data-collection-on-mturk-and-how-to-stop-it/}
}

@article{dennis2020online,
  title={Online worker fraud and evolving threats to the integrity of {MTurk} data: A discussion of virtual private servers and the limitations of {IP}-based screening procedures},
  author={Dennis, Sean A and Goodson, Brian M and Pearson, Christopher A},
  journal={Behavioral Research in Accounting},
  volume={32},
  number={1},
  pages={119--134},
  year={2020},
  publisher={American Accounting Association}
}

@article{kennedy2020shape,
  title={The shape of and solutions to the {MTurk} quality crisis},
  author={Kennedy, Ryan and Clifford, Scott and Burleigh, Tyler and Waggoner, Philip D and Jewell, Ryan and Winter, Nicholas JG},
  journal={Political Science Research and Methods},
  volume={8},
  number={4},
  pages={614--629},
  year={2020},
  publisher={Cambridge University Press}
}

@article{mellis2020mechanical,
  title={Mechanical {T}urk data collection in addiction research: Utility, concerns and best practices},
  author={Mellis, Alexandra M and Bickel, Warren K},
  journal={Addiction},
  volume={115},
  number={10},
  pages={1960--1968},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{saravanos2021hidden,
  title={The hidden cost of using {A}mazon {M}echanical {T}urk for research},
  author={Saravanos, Antonios and Zervoudakis, Stavros and Zheng, Dongnanzi and Stott, Neil and Hawryluk, Bohdan and Delfino, Donatella},
  booktitle={International Conference on Human-Computer Interaction},
  pages={147--164},
  year={2021},
  organization={Springer}
}

@inproceedings{jiang2021assessing,
  title={Assessing generalization of {SGD} via disagreement},
  author={Jiang, Yiding and Nagarajan, Vaishnavh and Baek, Christina and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@inproceedings{watson2022agree,
  title={Agree to disagree: When deep learning models with identical architectures produce distinct explanations},
  author={Watson, Matthew and Hasan, Bashar Awwad Shiekh and Al Moubayed, Noura},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={875--884},
  year={2022}
}

@article{bell2021recon,
  title={The {R}econ {A}pproach: A New Direction for Machine Learning in Criminal Law},
  author={Bell, Kristen and Hong, Jenny and McKeown, Nick and Voss, Catalin},
  journal={Berkeley Technology Law Journal},
  volume={37},
  year={2021}
}


@article{zhu2022bert,
  title={Is {BERT} Robust to Label Noise? {A} Study on Learning with Noisy Labels in Text Classification},
  author={Zhu, Dawei and Hedderich, Michael A and Zhai, Fangzhou and Adelani, David Ifeoluwa and Klakow, Dietrich},
  journal={arXiv:2204.09371},
  year={2022}
}

@article{samuel2022dark,
  title={The Dark Side of Sentiment Analysis: An Exploratory Review Using Lexicons, Dictionaries, and a Statistical Monkey and Chimp.},
  author={Samuel, Jim and Rozzi, Gavin and Palle, Ratnakar},
  journal={Dictionaries, and a Statistical Monkey and Chimp.(January 6, 2022)},
  year={2022}
}

@article{hendrycks2021cuad,
      title={{CUAD}: An Expert-Annotated NLP Dataset for Legal Contract Review}, 
      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},
      journal={NeurIPS},
      year={2021}
}

@inproceedings{muller2019identifying,
  title={Identifying mislabeled instances in classification datasets},
  author={M{\"u}ller, Nicolas M and Markert, Karla},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{abedjan2016detecting,
  title={Detecting data errors: Where are we and what needs to be done?},
  author={Abedjan, Ziawasch and Chu, Xu and Deng, Dong and Fernandez, Raul Castro and Ilyas, Ihab F and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael and Tang, Nan},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={12},
  pages={993--1004},
  year={2016},
  publisher={VLDB Endowment}
}

@article{kreutzer2022quality,
  title={Quality at a glance: An audit of web-crawled multilingual datasets},
  author={Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={50--72},
  year={2022},
  publisher={MIT Press}
}

@article{redman1998impact,
  title={The impact of poor data quality on the typical enterprise},
  author={Redman, Thomas C},
  journal={Communications of the ACM},
  volume={41},
  number={2},
  pages={79--82},
  year={1998},
  publisher={ACM New York, NY, USA}
}

@inproceedings{bauerle2022symphony,
  title={Symphony: Composing Interactive Interfaces for Machine Learning},
  author={B{\"a}uerle, Alex and Cabrera, {\'A}ngel Alexander and Hohman, Fred and Maher, Megan and Koski, David and Suau, Xavier and Barik, Titus and Moritz, Dominik},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2022}
}

@inproceedings{malik2011automatic,
  title={Automatic training data cleaning for text classification},
  author={Malik, Hassan H and Bhardwaj, Vikas S},
  booktitle={2011 IEEE 11th international conference on data mining workshops},
  pages={442--449},
  year={2011},
  organization={IEEE}
}
@article{reed2014training,
  title={Training deep neural networks on noisy labels with bootstrapping},
  author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
  journal={arXiv preprint arXiv:1412.6596},
  year={2014}
}

@inproceedings{joulin2016learning,
  title={Learning visual features from large weakly supervised data},
  author={Joulin, Armand and Maaten, Laurens van der and Jabri, Allan and Vasilache, Nicolas},
  booktitle={European Conference on Computer Vision},
  pages={67--84},
  year={2016},
  organization={Springer}
}

@article{hendrycks17baseline,
  author    = {Dan Hendrycks and Kevin Gimpel},
  title     = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
  journal = {Proceedings of International Conference on Learning Representations},
  year = {2017},
}

@article{zhang2017consensus,
  title={Consensus algorithms for biased labeling in crowdsourcing},
  author={Zhang, Jing and Sheng, Victor S and Li, Qianmu and Wu, Jian and Wu, Xindong},
  journal={Information Sciences},
  volume={382},
  pages={254--273},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{snow2008cheap,
  title={Cheap and fast--but is it good? evaluating non-expert annotations for natural language tasks},
  author={Snow, Rion and O’connor, Brendan and Jurafsky, Dan and Ng, Andrew Y},
  booktitle={Proceedings of the 2008 conference on empirical methods in natural language processing},
  pages={254--263},
  year={2008}
}

@inproceedings{snli,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{he2021debertav3,
  title={{DeBERTaV3}: Improving {DeBERTa} using {E}lectra-style pre-training with gradient-disentangled embedding sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv:2111.09543},
  year={2021}
}

@article{saito2015precision,
  title={The precision-recall plot is more informative than the {ROC} plot when evaluating binary classifiers on imbalanced datasets},
  author={Saito, Takaya and Rehmsmeier, Marc},
  journal={PloS one},
  volume={10},
  number={3},
  pages={e0118432},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{bertscore,
  title={{BERTS}core: Evaluating Text Generation with {BERT}},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{chen2021beyond,
  title={Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise},
  author={Chen, Pengfei and Ye, Junjie and Chen, Guangyong and Zhao, Jingwei and Heng, Pheng-Ann},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={11442--11450},
  year={2021}
}

@inproceedings{wei2022learning,
title={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},
author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TBWA6PLJZQm}
}


@inproceedings{plank2014linguistically,
  title={Linguistically debatable or just plain wrong?},
  author={Plank, Barbara and Hovy, Dirk and S{\o}gaard, Anders},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={507--511},
  year={2014}
}

@inproceedings{snow-etal-2008-cheap,
    title = "Cheap and Fast {--} But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
    author = "Snow, Rion  and
      O{'}Connor, Brendan  and
      Jurafsky, Daniel  and
      Ng, Andrew",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1027",
    pages = "254--263",
}

@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}

@article{dawson2021rethinking,
  title={Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness},
  author={Dawson, Glenn and Polikar, Robi},
  journal={arXiv preprint arXiv:2105.14083},
  year={2021}
}

@inproceedings{varshney-etal-2022-ildae,
    title = "{ILDAE}: Instance-Level Difficulty Analysis of Evaluation Data",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.240",
    doi = "10.18653/v1/2022.acl-long.240",
    pages = "3412--3425",
    abstract = "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students{'} potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5{\%} instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2{\%} higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
}

@article{hendrycks2016adjusting,
  title={Adjusting for dropout variance in batch normalization and weight initialization},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv:1607.02488},
  year={2016}
}

@inproceedings{owoputi-etal-2013-improved,
    title = "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters",
    author = "Owoputi, Olutobi  and
      O{'}Connor, Brendan  and
      Dyer, Chris  and
      Gimpel, Kevin  and
      Schneider, Nathan  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1039",
    pages = "380--390",
}